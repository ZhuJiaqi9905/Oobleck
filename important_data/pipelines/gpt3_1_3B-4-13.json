{"model": "gpt3_1_3B", "microbatch_size": 4, "world_size": 13, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [1], "6": [1], "7": [1], "8": [1], "9": [1], "10": [1], "11": [1], "12": [1], "13": [1], "14": [1], "15": [1], "16": [2], "17": [2], "18": [2], "19": [2], "20": [2], "21": [2], "22": [2], "23": [2], "24": [2], "25": [2]}, "num_of_microbatches": 62, "num_layers_per_stage": [5, 11, 10]}, {"layers": {"0": [3], "1": [3], "2": [4], "3": [4], "4": [4], "5": [4], "6": [4], "7": [4], "8": [4], "9": [5], "10": [5], "11": [5], "12": [5], "13": [5], "14": [5], "15": [6], "16": [6], "17": [6], "18": [6], "19": [6], "20": [6], "21": [7], "22": [7], "23": [7], "24": [7], "25": [7]}, "num_of_microbatches": 97, "num_layers_per_stage": [2, 7, 6, 6, 5]}, {"layers": {"0": [8], "1": [8], "2": [9], "3": [9], "4": [9], "5": [9], "6": [9], "7": [9], "8": [9], "9": [10], "10": [10], "11": [10], "12": [10], "13": [10], "14": [10], "15": [11], "16": [11], "17": [11], "18": [11], "19": [11], "20": [11], "21": [12], "22": [12], "23": [12], "24": [12], "25": [12]}, "num_of_microbatches": 97, "num_layers_per_stage": [2, 7, 6, 6, 5]}]}