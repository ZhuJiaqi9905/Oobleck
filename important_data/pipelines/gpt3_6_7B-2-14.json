{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 14, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [1], "5": [1], "6": [1], "7": [2], "8": [2], "9": [3], "10": [3], "11": [4], "12": [4], "13": [5], "14": [5], "15": [6], "16": [6], "17": [6], "18": [7], "19": [7], "20": [7], "21": [8], "22": [8], "23": [9], "24": [9], "25": [10], "26": [10], "27": [11], "28": [11], "29": [11], "30": [12], "31": [12], "32": [13], "33": [13]}, "num_of_microbatches": 512, "num_layers_per_stage": [4, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2]}]}