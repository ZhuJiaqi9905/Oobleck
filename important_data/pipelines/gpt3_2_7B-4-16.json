{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 16, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [1], "7": [1], "8": [1], "9": [1], "10": [1], "11": [2], "12": [2], "13": [2], "14": [2], "15": [3], "16": [3], "17": [3], "18": [3], "19": [4], "20": [4], "21": [4], "22": [4], "23": [5], "24": [5], "25": [5], "26": [5], "27": [6], "28": [6], "29": [6], "30": [6], "31": [7], "32": [7], "33": [7]}, "num_of_microbatches": 128, "num_layers_per_stage": [6, 5, 4, 4, 4, 4, 4, 3]}, {"layers": {"0": [8], "1": [8], "2": [8], "3": [8], "4": [8], "5": [8], "6": [9], "7": [9], "8": [9], "9": [9], "10": [9], "11": [10], "12": [10], "13": [10], "14": [10], "15": [11], "16": [11], "17": [11], "18": [11], "19": [12], "20": [12], "21": [12], "22": [12], "23": [13], "24": [13], "25": [13], "26": [13], "27": [14], "28": [14], "29": [14], "30": [14], "31": [15], "32": [15], "33": [15]}, "num_of_microbatches": 128, "num_layers_per_stage": [6, 5, 4, 4, 4, 4, 4, 3]}]}