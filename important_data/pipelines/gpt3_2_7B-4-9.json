{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 9, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [1], "5": [1], "6": [1], "7": [1], "8": [2], "9": [2], "10": [2], "11": [2], "12": [3], "13": [3], "14": [3], "15": [3], "16": [4], "17": [4], "18": [4], "19": [4], "20": [5], "21": [5], "22": [5], "23": [5], "24": [6], "25": [6], "26": [6], "27": [6], "28": [7], "29": [7], "30": [7], "31": [7], "32": [8], "33": [8]}, "num_of_microbatches": 256, "num_layers_per_stage": [4, 4, 4, 4, 4, 4, 4, 4, 2]}]}