{"model": "gpt3_1_3B", "microbatch_size": 4, "world_size": 16, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [0], "7": [0], "8": [1], "9": [1], "10": [1], "11": [1], "12": [1], "13": [1], "14": [2], "15": [2], "16": [2], "17": [2], "18": [2], "19": [2], "20": [3], "21": [3], "22": [3], "23": [3], "24": [3], "25": [3]}, "num_of_microbatches": 64, "num_layers_per_stage": [8, 6, 6, 6]}, {"layers": {"0": [4], "1": [4], "2": [4], "3": [4], "4": [4], "5": [4], "6": [4], "7": [4], "8": [5], "9": [5], "10": [5], "11": [5], "12": [5], "13": [5], "14": [6], "15": [6], "16": [6], "17": [6], "18": [6], "19": [6], "20": [7], "21": [7], "22": [7], "23": [7], "24": [7], "25": [7]}, "num_of_microbatches": 64, "num_layers_per_stage": [8, 6, 6, 6]}, {"layers": {"0": [8], "1": [8], "2": [8], "3": [8], "4": [8], "5": [8], "6": [8], "7": [8], "8": [9], "9": [9], "10": [9], "11": [9], "12": [9], "13": [9], "14": [10], "15": [10], "16": [10], "17": [10], "18": [10], "19": [10], "20": [11], "21": [11], "22": [11], "23": [11], "24": [11], "25": [11]}, "num_of_microbatches": 64, "num_layers_per_stage": [8, 6, 6, 6]}, {"layers": {"0": [12], "1": [12], "2": [12], "3": [12], "4": [12], "5": [12], "6": [12], "7": [12], "8": [13], "9": [13], "10": [13], "11": [13], "12": [13], "13": [13], "14": [14], "15": [14], "16": [14], "17": [14], "18": [14], "19": [14], "20": [15], "21": [15], "22": [15], "23": [15], "24": [15], "25": [15]}, "num_of_microbatches": 64, "num_layers_per_stage": [8, 6, 6, 6]}]}