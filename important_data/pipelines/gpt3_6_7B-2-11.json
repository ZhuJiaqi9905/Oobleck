{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 11, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [1], "5": [1], "6": [1], "7": [2], "8": [2], "9": [2], "10": [3], "11": [3], "12": [3], "13": [4], "14": [4], "15": [4], "16": [5], "17": [5], "18": [5], "19": [6], "20": [6], "21": [6], "22": [7], "23": [7], "24": [7], "25": [8], "26": [8], "27": [8], "28": [9], "29": [9], "30": [9], "31": [10], "32": [10], "33": [10]}, "num_of_microbatches": 512, "num_layers_per_stage": [4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}]}