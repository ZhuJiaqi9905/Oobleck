{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 10, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [1], "6": [1], "7": [1], "8": [1], "9": [2], "10": [2], "11": [2], "12": [3], "13": [3], "14": [3], "15": [4], "16": [4], "17": [4], "18": [5], "19": [5], "20": [5], "21": [6], "22": [6], "23": [6], "24": [7], "25": [7], "26": [7], "27": [8], "28": [8], "29": [8], "30": [8], "31": [9], "32": [9], "33": [9]}, "num_of_microbatches": 512, "num_layers_per_stage": [5, 4, 3, 3, 3, 3, 3, 3, 4, 3]}]}