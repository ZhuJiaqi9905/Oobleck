{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 11, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [0], "7": [0], "8": [1], "9": [1], "10": [1], "11": [1], "12": [1], "13": [1], "14": [1], "15": [2], "16": [2], "17": [2], "18": [2], "19": [2], "20": [2], "21": [2], "22": [3], "23": [3], "24": [3], "25": [3], "26": [3], "27": [3], "28": [3], "29": [4], "30": [4], "31": [4], "32": [4], "33": [4]}, "num_of_microbatches": 117, "num_layers_per_stage": [8, 7, 7, 7, 5]}, {"layers": {"0": [5], "1": [5], "2": [5], "3": [5], "4": [5], "5": [5], "6": [5], "7": [6], "8": [6], "9": [6], "10": [6], "11": [6], "12": [6], "13": [7], "14": [7], "15": [7], "16": [7], "17": [7], "18": [7], "19": [8], "20": [8], "21": [8], "22": [8], "23": [8], "24": [8], "25": [9], "26": [9], "27": [9], "28": [9], "29": [9], "30": [10], "31": [10], "32": [10], "33": [10]}, "num_of_microbatches": 139, "num_layers_per_stage": [7, 6, 6, 6, 5, 4]}]}