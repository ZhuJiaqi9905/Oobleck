{"model": "gpt3_1_3B", "microbatch_size": 4, "world_size": 11, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [0], "7": [0], "8": [0], "9": [0], "10": [1], "11": [1], "12": [1], "13": [1], "14": [1], "15": [1], "16": [1], "17": [1], "18": [2], "19": [2], "20": [2], "21": [2], "22": [2], "23": [2], "24": [2], "25": [2]}, "num_of_microbatches": 70, "num_layers_per_stage": [10, 8, 8]}, {"layers": {"0": [3], "1": [3], "2": [3], "3": [3], "4": [3], "5": [3], "6": [3], "7": [3], "8": [4], "9": [4], "10": [4], "11": [4], "12": [4], "13": [4], "14": [5], "15": [5], "16": [5], "17": [5], "18": [5], "19": [5], "20": [6], "21": [6], "22": [6], "23": [6], "24": [6], "25": [6]}, "num_of_microbatches": 93, "num_layers_per_stage": [8, 6, 6, 6]}, {"layers": {"0": [7], "1": [7], "2": [7], "3": [7], "4": [7], "5": [7], "6": [7], "7": [7], "8": [8], "9": [8], "10": [8], "11": [8], "12": [8], "13": [8], "14": [9], "15": [9], "16": [9], "17": [9], "18": [9], "19": [9], "20": [10], "21": [10], "22": [10], "23": [10], "24": [10], "25": [10]}, "num_of_microbatches": 93, "num_layers_per_stage": [8, 6, 6, 6]}]}