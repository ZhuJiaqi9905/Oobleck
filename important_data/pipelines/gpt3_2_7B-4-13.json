{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 13, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [0], "7": [1], "8": [1], "9": [1], "10": [1], "11": [1], "12": [1], "13": [2], "14": [2], "15": [2], "16": [2], "17": [2], "18": [2], "19": [3], "20": [3], "21": [3], "22": [3], "23": [3], "24": [4], "25": [4], "26": [4], "27": [4], "28": [4], "29": [5], "30": [5], "31": [5], "32": [5], "33": [5]}, "num_of_microbatches": 117, "num_layers_per_stage": [7, 6, 6, 5, 5, 5]}, {"layers": {"0": [6], "1": [6], "2": [6], "3": [6], "4": [6], "5": [6], "6": [7], "7": [7], "8": [7], "9": [7], "10": [7], "11": [8], "12": [8], "13": [8], "14": [8], "15": [8], "16": [9], "17": [9], "18": [9], "19": [9], "20": [9], "21": [10], "22": [10], "23": [10], "24": [10], "25": [10], "26": [11], "27": [11], "28": [11], "29": [11], "30": [12], "31": [12], "32": [12], "33": [12]}, "num_of_microbatches": 139, "num_layers_per_stage": [6, 5, 5, 5, 5, 4, 4]}]}