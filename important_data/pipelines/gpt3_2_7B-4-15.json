{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 15, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [1], "7": [1], "8": [1], "9": [1], "10": [1], "11": [2], "12": [2], "13": [2], "14": [2], "15": [2], "16": [3], "17": [3], "18": [3], "19": [3], "20": [3], "21": [4], "22": [4], "23": [4], "24": [4], "25": [4], "26": [5], "27": [5], "28": [5], "29": [5], "30": [6], "31": [6], "32": [6], "33": [6]}, "num_of_microbatches": 124, "num_layers_per_stage": [6, 5, 5, 5, 5, 4, 4]}, {"layers": {"0": [7], "1": [7], "2": [7], "3": [7], "4": [7], "5": [7], "6": [8], "7": [8], "8": [8], "9": [8], "10": [8], "11": [9], "12": [9], "13": [9], "14": [9], "15": [10], "16": [10], "17": [10], "18": [10], "19": [11], "20": [11], "21": [11], "22": [11], "23": [12], "24": [12], "25": [12], "26": [12], "27": [13], "28": [13], "29": [13], "30": [13], "31": [14], "32": [14], "33": [14]}, "num_of_microbatches": 132, "num_layers_per_stage": [6, 5, 4, 4, 4, 4, 4, 3]}]}