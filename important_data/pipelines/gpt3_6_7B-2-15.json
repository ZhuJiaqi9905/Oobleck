{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 15, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [1], "4": [1], "5": [1], "6": [2], "7": [2], "8": [2], "9": [3], "10": [3], "11": [3], "12": [4], "13": [4], "14": [5], "15": [5], "16": [6], "17": [6], "18": [7], "19": [7], "20": [8], "21": [8], "22": [9], "23": [9], "24": [10], "25": [10], "26": [11], "27": [11], "28": [12], "29": [12], "30": [13], "31": [13], "32": [14], "33": [14]}, "num_of_microbatches": 512, "num_layers_per_stage": [3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}]}