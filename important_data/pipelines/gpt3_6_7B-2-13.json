{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 13, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [1], "5": [1], "6": [1], "7": [2], "8": [2], "9": [2], "10": [3], "11": [3], "12": [3], "13": [4], "14": [4], "15": [4], "16": [5], "17": [5], "18": [5], "19": [6], "20": [6], "21": [7], "22": [7], "23": [8], "24": [8], "25": [8], "26": [9], "27": [9], "28": [10], "29": [10], "30": [11], "31": [11], "32": [12], "33": [12]}, "num_of_microbatches": 512, "num_layers_per_stage": [4, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2]}]}