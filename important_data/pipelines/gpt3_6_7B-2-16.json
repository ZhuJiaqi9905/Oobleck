{"model": "gpt3_6_7B", "microbatch_size": 2, "world_size": 16, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [1], "4": [1], "5": [1], "6": [2], "7": [2], "8": [3], "9": [3], "10": [4], "11": [4], "12": [5], "13": [5], "14": [6], "15": [6], "16": [7], "17": [7], "18": [8], "19": [8], "20": [9], "21": [9], "22": [10], "23": [10], "24": [11], "25": [11], "26": [12], "27": [12], "28": [13], "29": [13], "30": [14], "31": [14], "32": [15], "33": [15]}, "num_of_microbatches": 512, "num_layers_per_stage": [3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}]}