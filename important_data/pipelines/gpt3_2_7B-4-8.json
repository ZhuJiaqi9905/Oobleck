{"model": "gpt3_2_7B", "microbatch_size": 4, "world_size": 8, "pipelines": [{"layers": {"0": [0], "1": [0], "2": [0], "3": [0], "4": [0], "5": [0], "6": [1], "7": [1], "8": [1], "9": [1], "10": [1], "11": [2], "12": [2], "13": [2], "14": [2], "15": [3], "16": [3], "17": [3], "18": [3], "19": [4], "20": [4], "21": [4], "22": [4], "23": [5], "24": [5], "25": [5], "26": [5], "27": [6], "28": [6], "29": [6], "30": [6], "31": [7], "32": [7], "33": [7]}, "num_of_microbatches": 256, "num_layers_per_stage": [6, 5, 4, 4, 4, 4, 4, 3]}]}