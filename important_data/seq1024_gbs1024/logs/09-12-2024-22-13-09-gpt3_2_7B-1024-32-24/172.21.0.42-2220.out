[2024-09-12 22:13:12,734] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/workspace/Oobleck/oobleck/__init__.py
node_index: 0
[2024-09-12 22:13:15,851] [WARNING] [agent.py:154:_launch_workers] Profile data for model gpt2 not found. Exception Error parsing json file: /workspace/Oobleck/tmp/profiles/gpt3_2_7B-32-24-1/mb32.json Launching profiler...
[2024-09-12 22:13:17,814] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-12 22:13:21,048] [INFO] [profiler.py:270:profile] Profiling model gpt2. rank 0, world_size 24, num_workers_per_node 1.
Dataset: model_name: gpt2, dataset_path: wikitext, dataset_name: wikitext-2-raw-v1, max_seq_length: 1024
[2024-09-12 22:13:23,115] [INFO] [profiler.py:276:profile] Dataset loaded
model_name: gpt2, model_tag: gpt3_2_7B
[2024-09-12 22:13:24,298] [INFO] [profiler.py:284:profile] model loaded
[2024-09-12 22:14:02,703] [INFO] [profiler.py:292:profile] init pg. master_addr: 172.21.0.42, master_port: 23456
[2024-09-12 22:14:07,375] [INFO] [profiler.py:306:profile] Profiling model execution latency.
[2024-09-12 22:14:07,376] [INFO] [profiler.py:54:profile_execution_layers] Profiling layer execution latency: 0 iteration
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/root/miniconda3/envs/oobleck/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/root/miniconda3/envs/oobleck/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Oobleck/oobleck/planning/profiler.py", line 307, in profile
    layer_execution_result = profiler.profile_execution_layers(args.job.microbatch_size)
  File "/workspace/Oobleck/oobleck/planning/profiler.py", line 81, in profile_execution_layers
    output = gpu_layer(*input)
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.68", line 17, in forward
    crossentropyloss_0 = self.crossentropyloss_0(view_387, view_388);  view_387 = view_388 = None
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1174, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/root/miniconda3/envs/oobleck/lib/python3.10/site-packages/torch/nn/functional.py", line 3029, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 15.78 GiB total capacity; 11.76 GiB already allocated; 3.03 GiB free; 11.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-09-12 22:14:22,976] [INFO] [agent.py:189:_launch_workers] in agent. my_ip_port 172.21.0.42:2220, node_ip_ports ['172.21.0.42:2220', '172.21.0.42:2221', '172.21.0.42:2222', '172.21.0.42:2223', '172.21.0.46:2220', '172.21.0.46:2221', '172.21.0.46:2222', '172.21.0.46:2223', '172.21.0.47:2220', '172.21.0.47:2221', '172.21.0.47:2222', '172.21.0.47:2223', '172.21.0.90:2220', '172.21.0.90:2221', '172.21.0.90:2222', '172.21.0.90:2223', '172.21.0.91:2220', '172.21.0.91:2221', '172.21.0.91:2222', '172.21.0.91:2223', '172.21.0.92:2220', '172.21.0.92:2221', '172.21.0.92:2222', '172.21.0.92:2223']
[2024-09-12 22:14:22,976] [INFO] [agent.py:192:_launch_workers] Launching worker 0...
